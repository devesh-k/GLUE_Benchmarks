{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference https://huggingface.co/learn/nlp-course/en/chapter7/6?fw=pt#training-a-causal-language-model-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f733f16e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.23.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets) (1.21.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.8/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2.26.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.0.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: jupyter in /opt/conda/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.2)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.3.0)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.29.4)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.8/site-packages (from jupyter) (5.5.2)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.4.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (21.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.4)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.8.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (26.0.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (7.1.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.8/site-packages (from ipykernel->jupyter) (1.8.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (5.1.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.5.9)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->jupyter) (2.0.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.2.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->jupyter) (0.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->jupyter) (3.6.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (0.12.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook->jupyter) (21.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel->jupyter) (3.0.6)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#installing some libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade jupyter ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7976cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch, transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import AutoModel, AutoConfig,AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "from torch import cuda\n",
    "import datetime\n",
    "import warnings\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install transformers bitsandbytes>=0.39.0 -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51f6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df42477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "#global params for training\n",
    "\n",
    "batch_size = 8\n",
    "epoch = 100\n",
    "min_text_len = 0\n",
    "# train_loss_list = []\n",
    "# val_loss_list =[]\n",
    "if cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "#print(device)\n",
    "import os\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "#os.environ[\"MKL_DEBUG_CPU_TYPE\"] = \"5\"\n",
    "context_length = None\n",
    "global_tr_loss = torch.inf\n",
    "global_val_loss = torch.inf\n",
    "#print(global_tr_loss)\n",
    "model_path = os.path.join(\"model\")\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5903ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loading the dataset as hugging face dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset =load_dataset('wikitext', 'wikitext-103-raw-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9181db6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspecting the dataset and the split\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6c8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating pandas dataframe for easy manipulations and analysis\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "df_val = pd.DataFrame(dataset['validation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1af3010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801350\n",
      "3760\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train))\n",
    "print(len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f95e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Valkyria Chronicles III = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senjō no Valkyria 3 : Unrecorded Chronicles (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The game began development in 2010 , carrying...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                                   \n",
       "1                     = Valkyria Chronicles III = \\n\n",
       "2                                                   \n",
       "3   Senjō no Valkyria 3 : Unrecorded Chronicles (...\n",
       "4   The game began development in 2010 , carrying..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db3351c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create a new column called 'length' on our dataframe to analyze the text\n",
    "\n",
    "df_train['length'] = df_train['text'].apply(lambda x: len(x))\n",
    "df_val['length'] = df_val['text'].apply(lambda x: len(x))\n",
    "df_test['length'] = df_test['text'].apply(lambda x: len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b5d4985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Valkyria Chronicles III = \\n</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senjō no Valkyria 3 : Unrecorded Chronicles (...</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The game began development in 2010 , carrying...</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  length\n",
       "0                                                          0\n",
       "1                     = Valkyria Chronicles III = \\n      30\n",
       "2                                                          0\n",
       "3   Senjō no Valkyria 3 : Unrecorded Chronicles (...     706\n",
       "4   The game began development in 2010 , carrying...     524"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9c135",
   "metadata": {},
   "source": [
    "## Analyzing the length of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a9d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the range of length in the train set is 7066 down to 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"the range of length in the train set is {max(df_train.length)} down to {min(df_train.length)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e552de9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Lets check the the distributions\n",
    "\n",
    "sns.histplot(df_train['length'], bins='auto')\n",
    "plt.title('Histogram of text lengths')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507502f",
   "metadata": {},
   "source": [
    "#### we see that majority of the text is between 0 and 2000. Since the maximum length of text the GPT model handles is 1024, let's see that distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28aa0fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0  100  200  300  400  500  600  700  800  900 1000]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "array = np.arange(0, 1024, step=100)\n",
    "print(array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3833d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(df_train['length'], bins= array)\n",
    "plt.title('Histogram with length of text from 0 to  tokenizer len')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55c26cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fewer than 7.369361867488272% of rows have text more than the length of tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Let us check the number of rows whose length > 1024(the defualt length that the tokenizer can process)\n",
    "exceed_tok_len = sum(df_train['length']> 1024)/len(df_train)*100\n",
    "print(f\"fewer than {exceed_tok_len}% of rows have text more than the length of tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5912e2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636321"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check for the number of rows where length is 0\n",
    "sum(df_train['length'] == min_text_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be0320",
   "metadata": {},
   "source": [
    "#### We notice that some of the rows in our dataframe have no text.\n",
    "Removing such rows and resetting the indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2453bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['length'].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa8fee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = df_val[df_val['length'].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d817f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[df_test['length'].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78a1d309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_train['length'] == min_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce7fda5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_val['length'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "babc1116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Valkyria Chronicles III = \\n</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senjō no Valkyria 3 : Unrecorded Chronicles (...</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The game began development in 2010 , carrying...</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It met with positive sales in Japan , and was...</td>\n",
       "      <td>574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>= = Gameplay = = \\n</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  length\n",
       "1                     = Valkyria Chronicles III = \\n      30\n",
       "3   Senjō no Valkyria 3 : Unrecorded Chronicles (...     706\n",
       "4   The game began development in 2010 , carrying...     524\n",
       "5   It met with positive sales in Japan , and was...     574\n",
       "7                                = = Gameplay = = \\n      19"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c10c6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting the index\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_val.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6fcd0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calc the average len of the text:\n",
    "mean_len = int(df_train.length.mean())\n",
    "print(mean_len)\n",
    "import math\n",
    "power = math.ceil(math.log2(mean_len))\n",
    "print(power)\n",
    "context_length = 2**power\n",
    "context_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "861ba305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test the tokenizer:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\", padding = True ,return_tensors = \"pt\" , truncate = True, max_length  = context_length ,return_overflowing_tokens=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c960f187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code\n",
    "#define a list of randomly generated text\n",
    "# x = list(np.random.randint(0,60000,10))\n",
    "# x\n",
    "# text_list = [df_train.iloc[x]['text'] for x in x]\n",
    "# text_list\n",
    "# outputs = tokenizer(\n",
    "#     text_list,\n",
    "#     truncation=True,\n",
    "#     max_length=context_length,\n",
    "#     return_overflowing_tokens=True,\n",
    "#     return_length=True,\n",
    "#     padding = True\n",
    "    \n",
    "# )\n",
    "\n",
    "# #print(f\"Output: {outputs}\")\n",
    "# #print(f\"Input IDs : {(outputs['input_ids'])}\")\n",
    "# print(f\"Input IDs len : {(outputs['input_ids'])}\")\n",
    "# print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "# print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01b649",
   "metadata": {},
   "source": [
    "### Data loaders and Dataset for batched training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d155525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt_old(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length = context_length ):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "                        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        print(f\"inside loader...idx ->{idx}\")\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        print(f\"length of text ->{len(text)}\")\n",
    "        print(f\"text ->{text}\")\n",
    "        encoded = tokenizer(text, truncation=True, max_length=context_length, return_overflowing_tokens=True, padding = 'max_length')\n",
    "        input_ids = torch.tensor(encoded['input_ids']).squeeze()\n",
    "        #print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "        \n",
    "        attention_mask = torch.tensor(encoded['attention_mask']).squeeze()\n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "        labels = input_ids.clone()\n",
    "        print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n",
    "        return input_ids, attention_mask, labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1904f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_pyt(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length = context_length ):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "                                \n",
    "    def __getitem__(self, idx):\n",
    "        #print(f\"inside loader...idx ->{idx}\")\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        #print(f\"length of text ->{len(text)}\")\n",
    "        #print(f\"text ->{text}\")\n",
    "        encodings = tokenizer(text, truncation=True, max_length= self.max_length, return_overflowing_tokens=True, padding = 'max_length',return_tensors='pt')\n",
    "        # check the length of the encoded list\n",
    "        \n",
    "        #x_dict['input_id'] = input_ids_list\n",
    "        #x_dict['attention_mask'] = input_ids_list\n",
    "                \n",
    "        #print(f\"x_dict = {x_dict}\")             \n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "        #labels = input_id_list\n",
    "        #input_ids = torch.tensor(input_id_list)\n",
    "        #attention_mask = torch.tensor(attention_mask_list)\n",
    "        #labels = torch.tensor(labels)\n",
    "        #print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n",
    "        #print(f\"encoding = {encodings}\")\n",
    "               \n",
    "        return encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        #return the length of the dataframe\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8788bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    x_dict = {}\n",
    "    #print(\"CUSTOM COllate\")\n",
    "    #print(f\"bacth = {batch}\")\n",
    "    input_ids_list = []\n",
    "    att_mask_list = [] \n",
    "    for item in batch:\n",
    "        input_ids_list.append(item['input_ids'])\n",
    "        att_mask_list.append(item['attention_mask'])\n",
    "    #attention_mask = [item['attention_mask'].squeeze(0) for item in batch]\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    #print(f\"len of input_id_list = {len(input_ids_list)}\")\n",
    "    #print(f\"len of attmask_list = {len(att_mask_list)}\")\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.eos_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(att_mask_list, batch_first=True, padding_value=0)\n",
    "    #print(f\"shape of input_id tensor post padding -{input_ids.shape}\")\n",
    "    #print(f\"shape of attention_masks tensor post padding -{attention_mask.shape}\")\n",
    "    x_dict['input_ids'] = input_ids\n",
    "    x_dict['attention_mask'] = attention_mask\n",
    "    \n",
    "    return x_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef76e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32e3760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_pyt(df_train,tokenizer = tokenizer)\n",
    "val_dataset = dataset_pyt(df_val,tokenizer = tokenizer)\n",
    "test_dataset = dataset_pyt(df_test,tokenizer = tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = batch_size, shuffle = True , num_workers = 0, pin_memory = False, collate_fn = custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset,batch_size = batch_size, shuffle = True , collate_fn = custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset,batch_size = batch_size, shuffle = False, collate_fn = custom_collate_fn)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12ddc9",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7daf50a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71d09144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/73948214/how-to-convert-a-pytorch-nn-module-into-a-huggingface-pretrainedmodel-object/74109727#74109727\n",
    "# this code is needed to save the HF model as pre-trained and use this model as inference\n",
    "# class MyConfig(PretrainedConfig):\n",
    "#     model_type = 'mymodel'\n",
    "#     def __init__(self, important_param=42, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.important_param = important_param\n",
    "\n",
    "# class MyModel(PreTrainedModel):\n",
    "#     config_class = MyConfig\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         self.config = config\n",
    "#         self.model = clf_model_drp()\n",
    "#     def forward(self, ids ,token_type,att_mask):\n",
    "#         return self.model(ids ,token_type,att_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49303c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e9ed5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def eval_model(val_loader, model, epoch , device = device,):\n",
    "    global global_val_loss\n",
    "    #m = nn.Softmax()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    e = epoch+1\n",
    "    val_loss_list = []\n",
    "    #criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    print(f\"inside validation data for epoch {e}\")\n",
    "    #y_hat_val_list = []\n",
    "    #y_val_list = []\n",
    "    \n",
    "    for ind,x_dict in enumerate in enumerate(val_loader):\n",
    "        id_list = x_dict['input_ids']\n",
    "        #print(f\"id_list{id_list}\")\n",
    "        ids = id_list.clone().detach().to(device = device)\n",
    "        att_list = x_dict['attention_mask']\n",
    "        att_mask = att_list.clone().detach().to(device= device)\n",
    "        labels = ids.clone().detach().to(device = device)\n",
    "        #predictions\n",
    "        #print(f\"input_ids device = {input_ids.device}\")\n",
    "        model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            \n",
    "    mean_val_loss = torch.mean(torch.tensor(val_loss_list))\n",
    "    if mean_val_loss < global_val_loss:\n",
    "        print(f\"Val loss has decreased -->reducing the global validation loss from {global_val_loss:.2f} to {mean_val_loss:.2f}\")\n",
    "        global_val_loss = mean_val_loss\n",
    "        print(f\" validation loss for epoch = {e} is {torch.mean(torch.tensor(val_loss_list)):.4f}\")\n",
    "        #print metrics and save the model\n",
    "        #y_hat_val = torch.cat(y_hat_val_list)\n",
    "        #y_val = torch.cat(y_val_list)\n",
    "        #acc_val = accuracy_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy())\n",
    "        #f1_val = f1_score(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), average='micro')\n",
    "        print(f\" epoch= {e} : mean val loss is {torch.mean(torch.tensor(mean_val_loss)):.4f} \")\n",
    "        #save the model\n",
    "        \n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.datetime.now()\n",
    "        # Extract date and time components\n",
    "        current_date = str(current_datetime.date())\n",
    "        current_time = str(current_datetime.time()).split('.')[0]\n",
    "        file_name = 'model'+ current_date+current_time+'_'+'.pth'\n",
    "        path = os.path.join(\"model\",file_name)\n",
    "        print(f\"saving the model {file_name}\")\n",
    "        #torch.save(model.state_dict(), path)\n",
    "        model.save_pretrained(path)\n",
    "        \n",
    "        #plot_confusion_matrix(y_val.cpu().numpy(), y_hat_val.cpu().numpy(), labels)\n",
    "    else:\n",
    "        print(f\"No improvement in validation loss-->epoch= {e} and global val loss is {global_val_loss:.2f}\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6aba55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader,val_loader,model,num_epoch = 5,device = device):\n",
    "    global global_tr_loss\n",
    "    model.train()\n",
    "    device = device\n",
    "    print(f\"inside train model. Device = {device}\")\n",
    "    optimizer = torch.optim.AdamW(params =  model.parameters(), lr= 5e-5)\n",
    "    model.to(device)\n",
    "    #m = nn.Softmax()\n",
    "    from transformers import get_linear_schedule_with_warmup\n",
    "    scheduler = transformers.get_cosine_schedule_with_warmup( optimizer= optimizer, num_warmup_steps =len(train_loader)*num_epoch*.1 ,num_training_steps= len(train_loader)*num_epoch,last_epoch = -1 )\n",
    "    \n",
    "    for i in range (num_epoch):\n",
    "        #y_hat_list =[]\n",
    "        #label_list = []\n",
    "        epoch_train_loss = []\n",
    "        for ind,x_dict in enumerate(train_loader):\n",
    "            #print(f\"x_dict = {x_dict}\")\n",
    "            id_list = x_dict['input_ids']\n",
    "            #print(f\"id_list{id_list}\")\n",
    "            ids = id_list.clone().detach().to(device = device)\n",
    "            att_list = x_dict['attention_mask']\n",
    "            att_mask = att_list.clone().detach().to(device= device)\n",
    "            labels = ids.clone().detach().to(device = device)\n",
    "            #predictions\n",
    "            #print(f\"input_ids device = {input_ids.device}\")\n",
    "            model_output = model(input_ids = ids ,attention_mask = att_mask, labels = labels)\n",
    "            #print(f\"model_output->{model_output}\")\n",
    "            #probs = m(logits)\n",
    "            #y_hat_list.append(torch.argmax(probs , dim = 1))\n",
    "            #label_list.append(torch.argmax(lab, dim = 1))\n",
    "            \n",
    "            #loss calculation                   \n",
    "            act_loss = model_output.loss\n",
    "            epoch_train_loss.append(act_loss)\n",
    "            #epoch_train_loss.append(act_loss.item())\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            act_loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(f\"current LR->{scheduler.get_last_lr()}\")\n",
    "            scheduler.step()\n",
    "            #print(f\"POST SCHEDULER.step LR->{scheduler.get_last_lr()}\")\n",
    "            \n",
    "        #batch processing complete    \n",
    "        mean_loss = torch.mean(torch.tensor(epoch_train_loss))\n",
    "        \n",
    "        if mean_loss < global_tr_loss:\n",
    "            print(f\"training loss has decreased---> reducing the global loss from {global_tr_loss:.2f} to {mean_loss:.2f}\")\n",
    "            global_tr_loss = mean_loss\n",
    "            print(f\" epoch= {i+1} and mean train loss is {torch.mean(torch.tensor(epoch_train_loss)):.2f}\")\n",
    "            #printing training metrices\n",
    "            #y_hat = torch.cat(y_hat_list)\n",
    "            #y = torch.cat(label_list)\n",
    "            #acc = accuracy_score(y.cpu().numpy(), y_hat.cpu().numpy())\n",
    "            #f1 = f1_score(y.cpu().numpy(), y_hat.cpu().numpy(), average='micro')\n",
    "            print(f\" epoch= {i+1} : mean train loss is {torch.mean(torch.tensor(epoch_train_loss)):.4f} \")\n",
    "            #checking validation metrices\n",
    "            eval_model(val_loader, model, epoch = i , device = device)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No improvement in training loss..the global training loss is -->{global_tr_loss:.2f} \")\n",
    "            print(f\" epoch= {i+1} and mean train loss is {torch.mean(torch.tensor(epoch_train_loss)):.2f}\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    return model\n",
    "        \n",
    "            \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7086de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ea31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train model. Device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "# config = MyConfig(42)\n",
    "# model = MyModel(config)\n",
    "\n",
    "#train_loader,optimizer,val_loader ,num_epoch = 100, model = clf_model()\n",
    "tr_model = train_model(train_loader, val_loader, model =  model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b413a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb4ccd2d",
   "metadata": {},
   "source": [
    "### Evaluating the models on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_train.iloc[400327]['text']\n",
    "print (t)\n",
    "print(len(t))\n",
    "encoded = tokenizer(t, truncation=True, max_length=context_length, return_overflowing_tokens=True, padding = 'max_length',return_length= True)\n",
    "print(encoded)\n",
    "input_ids = torch.tensor(encoded['input_ids'])\n",
    "print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "\n",
    "attention_mask = torch.tensor(encoded['attention_mask'])\n",
    "#       print(f\"inside the loader and input_id = {input_ids} and its shape is {input_ids.shape}\")\n",
    "labels = input_ids.clone()\n",
    "print(f\"inside the loader and input_id shape= {input_ids.shape} attention_mask_shape is {attention_mask.shape} and label shape is {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_pred(loader, model, device = device,):\n",
    "    m = nn.Softmax()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    test_loss_list = []\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    y_hat_test_list = []\n",
    "    y_test_list = []\n",
    "    for ind,(x_dict, label_vec) in enumerate(loader):\n",
    "        model.to(device)\n",
    "        id_list = x_dict['id']\n",
    "        ids = torch.squeeze(torch.tensor(id_list, device = device),dim = 1).clone().detach()\n",
    "        tok_type_list = x_dict['token_type']\n",
    "        token_type = torch.squeeze(torch.tensor(tok_type_list, device = device),dim = 1).clone().detach()\n",
    "        att_list = x_dict['attention_mask']\n",
    "        att_mask = torch.squeeze(torch.tensor(att_list, device = device),dim = 1).clone().detach()\n",
    "        lab = label_vec.to(torch.device('cuda:0'))\n",
    "        logits = model(ids ,token_type,att_mask)\n",
    "        \n",
    "        probs = m(logits)\n",
    "        y_hat_test_list.append(torch.argmax(probs , dim = 1))\n",
    "        y_test_list.append(torch.argmax(lab , dim = 1))\n",
    "        \n",
    "        act_loss = criterion(logits, lab)\n",
    "        test_loss_list.append(act_loss.item())\n",
    "            \n",
    "    mean_test_loss = torch.mean(torch.tensor(test_loss_list))\n",
    "    print(f\" Test loss is {torch.mean(torch.tensor(test_loss_list)):.4f}\")\n",
    "    #print metrics and save the model\n",
    "    y_hat_test = torch.cat(y_hat_test_list)\n",
    "    y_test = torch.cat(y_test_list)\n",
    "    acc_test = accuracy_score(y_test.cpu().numpy(), y_hat_test.cpu().numpy())\n",
    "    f1_test = f1_score(y_test.cpu().numpy(), y_hat_test.cpu().numpy(), average='micro')\n",
    "    print(f\"mean loss is {torch.mean(torch.tensor(mean_test_loss)):.4f} -> the accuracy is {acc_test:.2f} ->the f1 is {f1_test:.2f} \")\n",
    "    #save the model\n",
    "    plot_confusion_matrix(y_test.cpu().numpy(), y_hat_test.cpu().numpy(), labels)\n",
    "\n",
    "   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9c4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = MyModel.from_pretrained('./model/model2024-05-1712:14:28_mrpc.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred(test_loader , saved_model )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d89fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
